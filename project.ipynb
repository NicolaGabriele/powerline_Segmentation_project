{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZE1u7IK/NSVEeyeHx3Y01",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicolaGabriele/powerline_Segmentation_project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k-rjJiRGqnN",
        "outputId": "27966318-b839-418f-8ab0-270fb1ae2be5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "BoLj9Lq0-vzR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import json\n",
        "TRAIN_DIR = '/content/drive/MyDrive/trainingset'\n",
        "TEST_DIR = '/content/drive/MyDrive/testset'\n",
        "TRAIN_LABELS = '/content/drive/MyDrive/train.json'\n",
        "TEST_LABELS =  '/content/drive/MyDrive/test.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PowerLineDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,img_dir,img_labels):\n",
        "    self.img_dir = img_dir\n",
        "    self.img_labels = json.load(open(img_labels))\n",
        "    self.categories = [] #lista delle etichette di classe\n",
        "    for c in self.img_labels['categories']:\n",
        "      self.categories.append(c['name'])\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels['images'][idx]['file_name'])\n",
        "    label = self.img_labels['annotations'][idx]\n",
        "    image = torchvision.io.read_image(img_path)\n",
        "    return image,label\n",
        "  def __len__(self):\n",
        "    return len(os.listdir(self.img_dir))"
      ],
      "metadata": {
        "id": "dFvKO65-C4IS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#caricamento dei dataset di train e di test\n",
        "train = PowerLineDataset(TRAIN_DIR, TRAIN_LABELS)\n",
        "test = PowerLineDataset(TEST_DIR, TEST_LABELS)\n",
        "train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5akcZwiMguY",
        "outputId": "31850f0a-f0ab-4ad0-b26b-d59347a4dc5e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[166, 177, 180,  ..., 186, 204, 174],\n",
              "          [167, 170, 177,  ..., 173, 188, 167],\n",
              "          [165, 167, 164,  ..., 167, 166, 173],\n",
              "          ...,\n",
              "          [169, 143, 144,  ...,  50,  54,  56],\n",
              "          [165, 193, 170,  ...,  50,  52,  55],\n",
              "          [176, 208, 196,  ...,  50,  52,  54]],\n",
              " \n",
              "         [[134, 145, 147,  ..., 166, 182, 153],\n",
              "          [135, 138, 144,  ..., 151, 166, 146],\n",
              "          [133, 135, 131,  ..., 145, 142, 150],\n",
              "          ...,\n",
              "          [164, 136, 132,  ...,  45,  46,  48],\n",
              "          [162, 186, 157,  ...,  45,  47,  50],\n",
              "          [173, 203, 180,  ...,  45,  47,  49]],\n",
              " \n",
              "         [[ 95, 107, 112,  ..., 116, 145, 122],\n",
              "          [ 96, 100, 109,  ..., 102, 129, 115],\n",
              "          [ 95,  97,  96,  ...,  98, 106, 119],\n",
              "          ...,\n",
              "          [ 70,  46,  50,  ...,  42,  44,  46],\n",
              "          [ 67,  96,  78,  ...,  42,  44,  47],\n",
              "          [ 78, 112, 103,  ...,  42,  44,  46]]], dtype=torch.uint8),\n",
              " {'segmentation': [[0.0,\n",
              "    125.74074074074075,\n",
              "    142.93323863636363,\n",
              "    106.79713804713805,\n",
              "    309.0672348484849,\n",
              "    84.70117845117846,\n",
              "    470.138134057971,\n",
              "    64.87117552334944,\n",
              "    612.0086050724638,\n",
              "    45.14492753623188,\n",
              "    699.8177083333334,\n",
              "    32.407407407407405,\n",
              "    699.8177083333334,\n",
              "    44.07407407407407,\n",
              "    589.0240036231884,\n",
              "    57.826086956521735,\n",
              "    430.87121212121207,\n",
              "    80.28198653198653,\n",
              "    222.89299242424244,\n",
              "    107.53367003367003,\n",
              "    95.703125,\n",
              "    123.73737373737374,\n",
              "    0.8285984848484849,\n",
              "    134.78535353535355]],\n",
              "  'iscrowd': 0,\n",
              "  'area': 7395.351122607652,\n",
              "  'image_id': 0,\n",
              "  'bbox': [0.0, 32.0, 699.0, 102.0],\n",
              "  'category_id': 0,\n",
              "  'id': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#questo metodo Ã¨ preso dai notebook e va adattato al caso specifico (loss ecc...)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "learning_rate = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_step = len(train)\n",
        "batch_size = 64\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train) for i in range(num_epochs + 1)]\n",
        "\n",
        "def train(epoch,model,criterion,optimizer,reshape=True):\n",
        "    for batch_idx, (images, labels) in enumerate(train):\n",
        "        # Move tensors to the configured device\n",
        "        if reshape:\n",
        "            images = images.reshape(-1, 28*28)\n",
        "        #images = images.to(device)\n",
        "        #labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch, num_epochs, batch_idx+1, total_step, loss.item()))\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        train_counter.append(\n",
        "        (batch_idx*batch_size) + ((epoch-1)*len(train)))"
      ],
      "metadata": {
        "id": "6YDig9ZXQBT_"
      },
      "execution_count": 63,
      "outputs": []
    }
  ]
}