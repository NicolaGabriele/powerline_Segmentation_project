{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicolaGabriele/powerline_Segmentation_project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**IMPORT DELLE LIBRERIE E CARICAMENTO DEL DATASET**\n"
      ],
      "metadata": {
        "id": "uPlvi52C5hXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK1cN188lTCY",
        "outputId": "a931e866-8ffe-4737-9aeb-2f3ce0e1bd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k5I2awhKlnZ6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.tv_tensors import BoundingBoxes, BoundingBoxFormat, Mask\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import json\n",
        "import random\n",
        "from pycocotools import mask\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "TRAIN_DIR = '/content/drive/MyDrive/trainingset' #'C:\\\\Users\\\\Nicola\\\\Documents\\\\GitHub\\\\powerline_Segmentation_project\\\\trainingset'#\n",
        "TEST_DIR = '/content/drive/MyDrive/testset' #'C:\\\\Users\\\\Nicola\\\\Documents\\\\GitHub\\\\powerline_Segmentation_project\\\\testset'#\n",
        "TRAIN_LABELS = '/content/drive/MyDrive/train.json' #'C:\\\\Users\\\\Nicola\\\\Documents\\\\GitHub\\\\powerline_Segmentation_project\\\\train.json'#\n",
        "TEST_LABELS =  '/content/drive/MyDrive/test.json' #'C:\\\\Users\\\\Nicola\\\\Documents\\\\GitHub\\\\powerline_Segmentation_project\\\\test.json'\n",
        "IM_WIDTH = 700\n",
        "IM_HEIGHT = 700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E-B8GMC3lrq6"
      },
      "outputs": [],
      "source": [
        "def groupByImageId(jsonfile):\n",
        "  data = jsonfile['annotations']\n",
        "  labs = {}\n",
        "  for k in data:\n",
        "    if k['image_id'] in labs:\n",
        "      labs[k['image_id']].append(k)\n",
        "    else:\n",
        "      labs[k['image_id']] = [k]\n",
        "  return labs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X56SZK7Yl7u4"
      },
      "outputs": [],
      "source": [
        "def decodeSegmentationField(segmentation):\n",
        "  return mask.decode(mask.merge(mask.frPyObjects(segmentation,IM_WIDTH,IM_HEIGHT)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formatLabels(labels):\n",
        "  bbox = []\n",
        "  seg = []\n",
        "  cls = []\n",
        "  i = 0\n",
        "  for label in labels:\n",
        "    seg.append(decodeSegmentationField(label['segmentation']))\n",
        "    b = label['bbox']\n",
        "    box = [b[0],b[1],b[0]+b[2], b[1]+b[3]]\n",
        "    if(box[2]-box[0] <= 0):\n",
        "      box[2]+=1\n",
        "    if(box[3]-box[1] <= 0):\n",
        "      box[3]+=1\n",
        "    bbox.append(box)\n",
        "    i+=1\n",
        "    cls.append(label['category_id']+1)\n",
        "  return  {\n",
        "      'boxes': torchvision.tv_tensors.BoundingBoxes(data=bbox,format = BoundingBoxFormat.XYXY, canvas_size=(IM_WIDTH, IM_HEIGHT)),\n",
        "      'labels': torch.tensor(cls, dtype = torch.int64),\n",
        "      'image_id': label['image_id'],\n",
        "      'area': torch.tensor(label['area'], dtype=torch.float64),\n",
        "      'iscrowd': torch.tensor(label['iscrowd'], dtype=torch.uint8),\n",
        "      'masks': Mask(seg)\n",
        "  }"
      ],
      "metadata": {
        "id": "cXTtmxRltmm4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ndYpml_vlw8E"
      },
      "outputs": [],
      "source": [
        "class PowerLineDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,img_dir,img_labels):\n",
        "    self.img_dir = img_dir\n",
        "    self.img_labels = json.load(open(img_labels))\n",
        "    self.categories = {} #lista delle etichette di classe\n",
        "    for c in self.img_labels['categories']:\n",
        "      self.categories[c['name']] = c['id']+1 #incrementiamo di uno per distinguere la background class\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels['images'][idx]['file_name'])\n",
        "    label = groupByImageId(self.img_labels)[idx]\n",
        "    image = (torchvision.io.read_image(img_path)/255)\n",
        "    #tra = torchvision.transforms.Resize((700,700))\n",
        "    return (image,formatLabels(label))\n",
        "  def __len__(self):\n",
        "    return len(os.listdir(self.img_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nPavYFFXl2FS"
      },
      "outputs": [],
      "source": [
        "#caricamento dei dataset di train e di test\n",
        "trainset = PowerLineDataset(TRAIN_DIR, TRAIN_LABELS)\n",
        "test = PowerLineDataset(TEST_DIR, TEST_LABELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**OPERAZIONI SUI DATI: DATA AUGMENTATION**"
      ],
      "metadata": {
        "id": "benlse6i62xT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsuwaV2d6r0Z"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, in_channels=3):\n",
        "    super(AutoEncoder,self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "         nn.Conv2d(in_channels, 64, kernel_size=3, padding = 1),\n",
        "         nn.ReLU(),\n",
        "         nn.Conv2d(64, 128, kernel_size=3, padding = 1),\n",
        "         nn.ReLU(),\n",
        "         nn.Conv2d(128, 256, kernel_size=3, padding = 1),\n",
        "         nn.ReLU(),\n",
        "         nn.MaxPool2d(2,stride = 2,return_indices=False))\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride = 2, padding = 1, output_padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 64, kernel_size=3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, in_channels, kernel_size=3, padding = 1),\n",
        "        nn.ReLU())\n",
        "\n",
        "  def forward(self,x):\n",
        "    #x.to('cuda')\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKDxlDQ79lNg"
      },
      "outputs": [],
      "source": [
        "model = AutoEncoder()\n",
        "#model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsKV11idmA0z"
      },
      "outputs": [],
      "source": [
        "#questo metodo Ã¨ preso dai notebook e va adattato al caso specifico (loss ecc...)\n",
        "#costruisco un sottodataset di 200 immagini\n",
        "#indexs = list(range(0,200))\n",
        "#trains = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset,indexs), batch_size=64, shuffle=True)\n",
        "trains = torch.utils.data.DataLoader(trainset)\n",
        "criterion = torch.nn.MSELoss()\n",
        "learning_rate = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_step = len(trains)\n",
        "batch_size = 64\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(trains) for i in range(num_epochs + 1)]\n",
        "\n",
        "\n",
        "def train(epoch,model,criterion,optimizer,reshape=False):\n",
        "    for batch_idx, (images, labels) in enumerate(trains):\n",
        "        # Move tensors to the configured device\n",
        "        #if reshape:\n",
        "         #   images = images.reshape(-1, 28*28)\n",
        "        #images = images.to('cuda')\n",
        "        #labels = labels.to('cuda')\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if random.random() < 0.1:\n",
        "          print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch, num_epochs, batch_idx+1, total_step, loss.item()))\n",
        "        if (batch_idx+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch, num_epochs, batch_idx+1, total_step, loss.item()))\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        train_counter.append(\n",
        "        (batch_idx*batch_size) + ((epoch-1)*len(trains)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl-6B8ezdvky"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/content/drive/MyDrive/autoencoder')\n",
        "#train(num_epochs,model,criterion, optimizer, reshape=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE5unNbvf4DP"
      },
      "outputs": [],
      "source": [
        "torch.save(model,'autoencoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LOSS FUNCTIONS**"
      ],
      "metadata": {
        "id": "fOUlmpO07mta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLossFunction(nn.Module):\n",
        "  def __init__(self, cls_loss, bb_loss, seg_loss, cls_weight, bb_weight, seg_weight):\n",
        "    super(CustomLossFunction, self).__init__()\n",
        "    self.cls_loss = cls_loss\n",
        "    self.bb_loss = bb_loss\n",
        "    self.seg_loss = seg_loss\n",
        "    self.cls_weight = cls_weight\n",
        "    self.bb_weight = bb_weight\n",
        "    self.seg_weight = seg_weight\n",
        "  def forward(self,predictions, targets):\n",
        "    prediction = predictions[0]\n",
        "    formatted_targets = formatLabels(targets)\n",
        "    #loss di classificazione\n",
        "    #scores di classificazione: uno per ogni oggetto predetto\n",
        "    targets_cls = formatted_targets['labels']\n",
        "    pred_cls = prediction['scores']\n",
        "    loss_cls = self.cls_loss(pred_cls,targets_cls)\n",
        "    #loss di bbox\n",
        "    targets_bbox = formatted_targets['boxes']\n",
        "    pred_bbox = prediction['boxes']\n",
        "    loss_bbox = self.bb_loss(pred_bbox,targets_bbox)\n",
        "    #loss per le maschere: sigmoide su tutti i pixel e poi cross entropy\n",
        "    targets_masks = formatted_targets['masks']\n",
        "    pred_masks = prediction['masks']\n",
        "    loss_masks = self.seg_loss(pred_masks,targets_masks)\n",
        "\n",
        "    return self.cls_weight*loss_cls + self.bb_weight*loss_bbox + self.seg_weight*loss_masks"
      ],
      "metadata": {
        "id": "gBLzLAje8GPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TEST DI UN MODELLO PREADDESTRATO: MASK R-CNN**"
      ],
      "metadata": {
        "id": "1xqBHACL7Pmb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KbbHfR2pTSNJ"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "y0rl6FXEQ3t2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "\n",
        "# Get the numbner of output channels for the Mask Predictor\n",
        "dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
        "\n",
        "# Replace the box predictor\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features_box, num_classes=5)\n",
        "\n",
        "# Replace the mask predictor\n",
        "#model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels=in_features_mask, dim_reduced=dim_reduced, num_classes=4)"
      ],
      "metadata": {
        "id": "EKT2pH78mXeD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3S6gc70uUA9U"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "#predictions = model([trainset[3][0]], [formatLabels(trainset[3][1])])\n",
        "learning_rate = 1e-5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "batch_size = 2\n",
        "trains = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle=False,collate_fn= collate_fn)\n",
        "total_step = len(trains)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(trains) for i in range(num_epochs + 1)]\n",
        "\n",
        "\n",
        "def train(epoch,model,optimizer):\n",
        "    for batch_idx, (images, targets) in enumerate(trains):\n",
        "        # Move tensors to the configured device\n",
        "        #if reshape:\n",
        "         #   images = images.reshape(-1, 28*28)\n",
        "        #images = images.to('cuda')\n",
        "        #labels = labels.to('cuda')\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        #backward pass\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx+1) % 1 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch, num_epochs, batch_idx+1, total_step, losses))\n",
        "\n",
        "        train_losses.append(losses)\n",
        "        train_counter.append(\n",
        "        (batch_idx*batch_size) + ((epoch-1)*len(trains)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(epoch=1, model = model, optimizer = optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "4_49YhcOc4GQ",
        "outputId": "bec016ac-373a-4ee7-fed2-957cf46752c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/tv_tensors/_tv_tensor.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  return torch.as_tensor(data, dtype=dtype, device=device).requires_grad_(requires_grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Step [1/421], Loss: 4.9923\n",
            "Epoch [1/1], Step [2/421], Loss: 7.7263\n",
            "Epoch [1/1], Step [3/421], Loss: 3.8354\n",
            "Epoch [1/1], Step [4/421], Loss: 5.4160\n",
            "Epoch [1/1], Step [5/421], Loss: 3.7608\n",
            "Epoch [1/1], Step [6/421], Loss: 2.9913\n",
            "Epoch [1/1], Step [7/421], Loss: 4.2554\n",
            "Epoch [1/1], Step [8/421], Loss: 5.6587\n",
            "Epoch [1/1], Step [9/421], Loss: 3.0140\n",
            "Epoch [1/1], Step [10/421], Loss: 1.8329\n",
            "Epoch [1/1], Step [11/421], Loss: 1.4331\n",
            "Epoch [1/1], Step [12/421], Loss: 2.1331\n",
            "Epoch [1/1], Step [13/421], Loss: 1.3558\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a00cddbee148>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-ad89e01688fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***CODICI DI PROVA***"
      ],
      "metadata": {
        "id": "O_hffNPAabZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = iter(trains)\n",
        "for j in range(0,5):\n",
        "  k = next(i)\n",
        "  print(k[1][37])\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s66k_WXGkDyN",
        "outputId": "8b05b813-4f59-4ef0-9c52-0a0fe95a0590"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'boxes': BoundingBoxes([[  0.,   1., 181., 642.],\n",
            "               [140.,   0., 278., 699.],\n",
            "               [351.,   0., 412., 699.],\n",
            "               [526.,   0., 556., 699.]], format=BoundingBoxFormat.XYXY, canvas_size=(700, 700)), 'labels': tensor([1, 1, 1, 1]), 'image_id': 37, 'area': tensor(4195.9834, dtype=torch.float64), 'iscrowd': tensor(0, dtype=torch.uint8), 'masks': Mask([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}\n",
            "\n",
            "\n",
            "{'boxes': BoundingBoxes([[  0., 545., 699., 629.],\n",
            "               [166.,   0., 699., 356.],\n",
            "               [575.,   0., 698., 138.],\n",
            "               [  0., 633., 606., 659.],\n",
            "               [  0., 642., 618., 675.],\n",
            "               [602., 647., 699., 699.],\n",
            "               [647., 668., 699., 699.],\n",
            "               [454., 498., 473., 588.],\n",
            "               [688., 488., 699., 543.]], format=BoundingBoxFormat.XYXY, canvas_size=(700, 700)), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 2, 2]), 'image_id': 101, 'area': tensor(353.2611, dtype=torch.float64), 'iscrowd': tensor(0, dtype=torch.uint8), 'masks': Mask([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      ...,\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}\n",
            "\n",
            "\n",
            "{'boxes': BoundingBoxes([[  0., 274., 699., 489.],\n",
            "               [  0.,   0., 622., 282.],\n",
            "               [  0.,   0., 302., 175.],\n",
            "               [  0.,   0., 234., 146.]], format=BoundingBoxFormat.XYXY, canvas_size=(700, 700)), 'labels': tensor([1, 1, 1, 1]), 'image_id': 165, 'area': tensor(668.7143, dtype=torch.float64), 'iscrowd': tensor(0, dtype=torch.uint8), 'masks': Mask([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}\n",
            "\n",
            "\n",
            "{'boxes': BoundingBoxes([[341.,   0., 354., 194.],\n",
            "               [335.,  25., 496., 449.],\n",
            "               [349., 253., 350., 266.],\n",
            "               [328., 277., 349., 699.],\n",
            "               [325., 269., 347., 699.],\n",
            "               [347., 201., 351., 248.],\n",
            "               [349., 202., 353., 249.],\n",
            "               [339., 199., 341., 217.],\n",
            "               [314., 228., 339., 699.],\n",
            "               [353., 125., 355., 146.],\n",
            "               [353., 125., 358., 196.],\n",
            "               [350., 162., 353., 195.],\n",
            "               [354.,   0., 362., 120.],\n",
            "               [357.,   0., 364., 120.],\n",
            "               [472.,   0., 473.,  35.],\n",
            "               [472.,  39., 473., 131.],\n",
            "               [489., 135., 490., 147.],\n",
            "               [487., 135., 488., 157.],\n",
            "               [489., 164., 490., 206.],\n",
            "               [487., 168., 488., 205.],\n",
            "               [489., 211., 490., 229.],\n",
            "               [472., 137., 472., 206.],\n",
            "               [471., 213., 472., 270.],\n",
            "               [471., 275., 472., 288.],\n",
            "               [466., 297., 472., 699.],\n",
            "               [484., 211., 488., 699.],\n",
            "               [486., 238., 490., 699.],\n",
            "               [626.,   0., 666., 699.],\n",
            "               [622.,   0., 661., 699.],\n",
            "               [489.,   0., 490., 131.],\n",
            "               [487.,   0., 488., 131.]], format=BoundingBoxFormat.XYXY, canvas_size=(700, 700)), 'labels': tensor([1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1]), 'image_id': 229, 'area': tensor(87.2213, dtype=torch.float64), 'iscrowd': tensor(0, dtype=torch.uint8), 'masks': Mask([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      ...,\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}\n",
            "\n",
            "\n",
            "{'boxes': BoundingBoxes([[351., 467., 699., 699.],\n",
            "               [  0., 169., 699., 571.],\n",
            "               [  0.,  62., 699., 440.],\n",
            "               [  0.,   0., 363., 163.]], format=BoundingBoxFormat.XYXY, canvas_size=(700, 700)), 'labels': tensor([1, 1, 1, 1]), 'image_id': 293, 'area': tensor(1993.8758, dtype=torch.float64), 'iscrowd': tensor(0, dtype=torch.uint8), 'masks': Mask([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "      [[0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0],\n",
            "       [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "im = torchvision.transforms.ToPILImage(trainset[1][0])\n",
        "prediction = model([trainset[1][0]])"
      ],
      "metadata": {
        "id": "HE7n1tfNQFN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "im = torchvision.transforms.ToPILImage()(trainset[1][0])\n",
        "\n",
        "# Create figure and axes\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Display the image\n",
        "ax.imshow(im)\n",
        "#239.0, 145.0, 273.0, 500.0\n",
        "# Create a Rectangle patch\n",
        "rect = patches.Rectangle((239, 145), 273, 500, linewidth=1, edgecolor='r', facecolor='none')\n",
        "rect2 = patches.Rectangle((239, 145), 273-239, 500-145, linewidth=1, edgecolor='b', facecolor='none')\n",
        "# Add the patch to the Axes\n",
        "ax.add_patch(rect)\n",
        "ax.add_patch(rect2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uCOzgGTTOFME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatLabels(trainset[1][1])"
      ],
      "metadata": {
        "id": "tUHZrXHrs949"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}